[["index.html", "Visualizing the Data Chapter 1 Cleaning a Dataset of U of I Library Database Usage and Institutional Variables 1.1 Motivation 1.2 Sources 1.3 End goals 1.4 Following along", " Visualizing the Data Norm Lee 2024-12-09 Chapter 1 Cleaning a Dataset of U of I Library Database Usage and Institutional Variables 1.1 Motivation The U of I Library is continually searching for ways to assess its services to the University community. However, the library does not have standardized workflows for collecting and synthesizing some University-level variables (e.g., incoming grant funding, department/college level enrollment, number and character of research outputs produced, etc.). Moving from informal to formal assessment of these type of variables may provide new opportunities for improvement. This site is a proof-of-concept for developing those workflows in R. Since this initiative is in its early stages, this site does not focus on making actual inferences from any data. Instead, the goal is to demonstrate how the data can be processed and cleaned for potential quasi-experimental or interpretivist inference. The project was not entirely successful; there are several flaws which this site will point out later. But - in my opinion - it was successful enough to demonstrate that such work is feasible given a properly formulated research question and realistic expectations of data quality and research design limitations. 1.2 Sources The “raw data” for this project includes: Spring and fall department enrollment numbers pulled from the U of I IR Internal Dashboards University research outputs data pulled from the Library’s VERSO platform Grant expenditure data from the U of I’s Research Reports Archive, and U of I Library database usage statistics pulled from our internal LibApps system. All of these sources are observational and incomplete in a number of ways. A complete evaluation of how that limits useful inference is outside the scope of this site. However, some of those issues will come up in data processing. 1.3 End goals The goals for this project were to: Take the raw data (retrieved as xlsx or csv’s) and clean it. From the cleaned raw data, generate a set of tidy data.frames for further analysis. Store those data.frames in a SQL database. Visualize data from the SQL database in R in order to suggest interesting assessment options. The most important shortcoming of this project was an error in (2). Two of the three final dataframes created were not tidy, one of which in a way that can’t easily be fixed and makes (4) significantly more difficult (4). 1.4 Following along Because some of the data included in this analysis was confidential, this document does not dynamically process any of it. However, I created an anonymized version of the data so readers can follow along if it’s helpful. To use it: Clone this repository. Extract the contents of the sample-raw-data.zip contained in the ‘raw_data’ directory into the ‘raw_data’ directory. From there, you can either run the entire analysis all at once using code/00-main.R, or follow along in R scripts 01-06. Note: you may need to install one or more of the required packages listed in the “Cleaning the Data” section of this site. "],["cleaning-the-data.html", "Chapter 2 Cleaning the Data 2.1 Importing Packages &amp; Data 2.2 Exploring the Data 2.3 Data Cleaning 2.4 The Enrollment Numbers Data.frame 2.5 The Grant Funding Data.frame", " Chapter 2 Cleaning the Data 2.1 Importing Packages &amp; Data First, we import a number of packages with convenient functions: library(tidyverse) # helps data processing library(DBI) # allow sql operations in R library(readxl) # allow creation of data.frame from xlsx library(cli) # color console outputs for easy reading library(lubridate) # helps date processing library(stringr) # help string/substring processing library(uuid) # generate unique identifiers library(ggplot2) # generate visualizations library(ggrepel) # labelling for ggplots Then we import the data: # Library database usage az_database_list &lt;- read.csv(&#39;raw_data/az_database_list.csv&#39;) az_database_usage &lt;- read.csv(&#39;raw_data/database_usage_by_month.csv&#39;) # U of I research outputs from VERSO research_outputs &lt;- read.csv(&#39;raw_data/research-outputs-by-ay-and-college.csv&#39;) # Enrollment numbers from RI dashboards spring_enrollment &lt;- read_xlsx(&#39;raw_data/spring-enrollment_by_college-major-and-semester.xlsx&#39;) fall_enrollment &lt;- read_xlsx(&#39;raw_data/fall-enrollment_by_college-major-and-semester.xlsx&#39;) # Grant expenditures from research report archives grant_funding &lt;- read_xlsx(&#39;raw_data/funding-reports-combined_by_college-and-fy.xlsx&#39;) 2.2 Exploring the Data Since the data is raw and messy the first thing we want to do is get a sense of the columns and data types in our new data.frames. View(az_database_list) # view data.frame for (i in colnames(az_database_list)){ # get list of columns and their data types. cat(col_blue(i), &#39;: &#39;, col_red(az_database_list[[i]]), &quot;\\n&quot;) # color-code the output for easy reading } View(az_database_usage) for (i in colnames(az_database_usage)){ cat(col_blue(i), &#39;: &#39;, col_red(az_database_usage[[i]]), &quot;\\n&quot;) } View(fall_enrollment) for (i in colnames(fall_enrollment)){ cat(col_blue(i), &#39;: &#39;, col_red(fall_enrollment[[i]]), &quot;\\n&quot;) } View(spring_enrollment) for (i in colnames(spring_enrollment)){ cat(col_blue(i), &#39;: &#39;, col_red(spring_enrollment[[i]]), &quot;\\n&quot;) } View(grant_funding) for (i in colnames(grant_funding)){ cat(col_blue(i), &#39;: &#39;, col_red(grant_funding[[i]]), &quot;\\n&quot;) } View(research_outputs) for (i in colnames(research_outputs)){ cat(col_blue(i), &#39;: &#39;, col_red(research_outputs[[i]]), &quot;\\n&quot;) } Technically, this didn’t need to be a for loop. But, as a Python native, at this point in the project I wasn’t used to R’s innate vectorization and the apply family functions. Now I leave them in to remember my roots and where I come from. :D 2.3 Data Cleaning With that done, we can start the data cleaning. First, by removing columns we obviously do not need and re-naming the ones we do need to avoid issues with special characters in the future: az_database_list &lt;- select(az_database_list, id = &quot;ID&quot;, name = &quot;Name&quot;, description = &quot;Description&quot;, date_created = &quot;Created&quot;, vendor = &quot;Vendor&quot;, subjects = &quot;Subjects&quot;) fall_enrollment &lt;- select(fall_enrollment, college = &quot;College&quot;, term = &quot;Term Description&quot;, enrollment = &quot;Count (Data suppression) 1&quot;) spring_enrollment &lt;- select(spring_enrollment, college = &quot;College&quot;, term = &quot;Term Description&quot;, enrollment = &quot;Count (Data suppression) 1&quot;) grant_funding &lt;- select(grant_funding, fiscal_year = &quot;fiscal-year&quot;, college, grant_proposals = &quot;Number of proposals&quot;, grant_funding = &quot;research expenditures&quot;) From there, we can begin cleaning the data sources one by one. The processing for each of these was done in one big pipe chain, but to make it more readable, I’ve broken it into discrete chunks. The original pipe chains can be found in 03-preprocess-raw-data.R. 2.3.1 The Research Outputs Data.frame. The ultimate goal for this data.frame is to have a data.frame that counts the number of publications each college published each fiscal year. So, our observational unit “Fiscal Year + College”. E.g., “The College of Engineering produced X publications in fiscal year 2020”. This was my first mistake with tidiness! Technically, concatenating fiscal year and college into one column is untidy. However, as we’ll see during the “visualization” section, this error does not actually make processing the data much more difficult. Within the research outputs data.frame, each research asset is affiliated with one or more colleges based on the affiliation(s) of the author(s). Unfortunately, each affiliation is listed in its own column. Based on looking at the data.frame colnames, we can tell assets have between 1 and 7 columns dedicated to their affiliations. We need to collapse those into one column that contains a vector of all the asset’s affiliations: print(unique(unlist(research_outputs[9:16]))) # used to determine whether empty cells contained &quot;&quot;, NA&#39;s, NULLs, or other values research_outputs &lt;- research_outputs %&gt;% mutate(unit_affiliations = lapply(1:nrow(research_outputs), function(i){ unique(research_outputs[i, 9:16][research_outputs[i, 9:16] != &quot;&quot;]) # != condition based on print statement above }) ) %&gt;% select(asset_id = &quot;Asset.Id&quot;, pub_year = &quot;Asset.Published.Date..String.&quot;, type = &quot;Asset.Type&quot;, unit_affiliations) # remove the 7 columns that were previously dedicated to asset affiliations ### # debug print(unique(unlist(research_outputs$unit_affiliations))) # check to make sure all expected Colleges are somewhere in the new unit affiliations column ### Next, we need to determine which fiscal year the assets were published in because many University level variables are reported based on fiscal year and we want the asset data to mesh with that. To do so, we need to clean the “pub_year” column, creating lubridate-friendly values. Unfortunately, dates are stored in a variety of formats: YYYY DD/MM/YYYY MM/DD/YYYY YY-MMM (where MMM is a 3 letter abbreviation such as “Jan”) MMM-YY Huge thanks to https://regex101.com/ and https://regex-generator.olafneumann.org/ for helping me create and debug the regular expressions that uniquely pick out the date formats seen in the data.frame! %&gt;% mutate( pub_year_cleaned = case_when( # use regex to parse different date formats into lubridate friendly strings str_detect(pub_year, &quot;^\\\\d{4}$&quot;) ~ mdy(paste0(&#39;01/01/&#39;,pub_year)), str_detect(pub_year, &quot;^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[0-2])/(\\\\d{4}$)&quot;) ~ dmy(pub_year), str_detect(pub_year, &quot;^([1-9]|1[12])/([1-9]|[12][0-9]|3[01])/(19|20)\\\\d{2}$&quot;) ~ mdy(pub_year), str_detect(pub_year, &quot;^\\\\d{1,2}-[a-zA-Z]{3}$&quot;) ~ ymd( paste0( str_replace(str_extract(pub_year, &quot;\\\\d{1,2}&quot;), &quot;\\\\d{1,2}&quot;, paste0(&#39;20&#39;,str_extract(pub_year, &quot;\\\\d{1,2}&quot;),&#39;-&#39;,str_extract(pub_year, &#39;[a-zA-Z]{3}&#39;),&#39;-&#39;,&#39;01&#39;)))), str_detect(pub_year, &quot;^[a-zA-Z]{3}-\\\\d{1,2}$&quot;) ~ ymd( paste0( str_replace(str_extract(pub_year, &quot;\\\\d{1,2}&quot;), &quot;\\\\d{1,2}&quot;, paste0(&#39;20&#39;,str_extract(pub_year, &quot;\\\\d{1,2}&quot;),&#39;-&#39;,str_extract(pub_year, &#39;[a-zA-Z]{3}&#39;),&#39;-&#39;,&#39;01&#39;)))), TRUE ~ as_date(pub_year) ), needs_fy_sim = case_when( # dummy column. For publications where only &quot;YYYY&quot; dates are provided there isn&#39;t enough info to determine fy, so must be simulated str_detect(pub_year, &quot;^\\\\d{4}$&quot;) ~ TRUE, str_detect(pub_year, &quot;^(0[1-9]|[12][0-9]|3[01])/(0[1-9]|1[0-2])/(\\\\d{4}$)&quot;) ~ FALSE, str_detect(pub_year, &quot;^([1-9]|1[12])/([1-9]|[12][0-9]|3[01])/(19|20)\\\\d{2}$&quot;) ~ FALSE, str_detect(pub_year, &quot;^\\\\d{1,2}-[a-zA-Z]{3}$&quot;) ~ FALSE, str_detect(pub_year, &quot;^[a-zA-Z]{3}-\\\\d{1,2}$&quot;) ~ FALSE ) ) %&gt;% mutate( random_number = runif(n()), # generate a column of random numbers fiscal_year = case_when( needs_fy_sim == TRUE &amp; random_number &lt; 0.5 ~ as.integer(year(pub_year_cleaned)), # randomly generate a fiscal year based on whether a randomly generated number is above or below 0.5 needs_fy_sim == TRUE &amp; random_number &gt;= 0.5 ~ as.integer(year(pub_year_cleaned))-1, needs_fy_sim == FALSE &amp; month(pub_year_cleaned) &gt;= 7 ~ as.integer(year(pub_year_cleaned)), # else, just extract month from lubridate dates, using a July - June fiscal year needs_fy_sim == FALSE &amp; month(pub_year_cleaned) &lt; 7 ~ as.integer(year(pub_year_cleaned))-1 )) Now, we run into a data limitation. If an asset is affiliated with multiple colleges within a particular fiscal year, how do we count it? Does each College get the credit for one entire output, or something else? Recall that the idea behind this project is - eventually - we may want to investigate how University variables relate to Library database usage. Because of that, we’re not actually interested in the research outputs themselves, they are just an imprecise proxy. Instead, we want some representation of the “effort” a researcher had to go through to publish that asset, “effort” which might have led them to access journal articles, conference papers, etc., through the Library’s databases. Consequently, it didn’t make much sense to simply assign each College one entire output, because presumably the “effort” of producing the asset is shared across authors. So, rather than assigning one asset per affiliated College, I assigned 1 / (number of units affiliated with the asset). So, if one professor from Engineering and one from Science co-authored a paper, each College would get 0.5 of an asset. Obviously, this 1 / (number of units affiliated with the asset) is only slightly more nuanced than just assigning 1 to each College. However, implementing it as code at least makes the assumptions and causal story I’m telling more explicit: %&gt;% mutate( count_unique_affiliations = sapply(unit_affiliations, function(i) length(i)), effort_per_affiliation = 1/count_unique_affiliations) Afterwards, there’s just some clean-up: %&gt;% na.omit(pub_year_cleaned) %&gt;% # remove research outputs without dates select(-random_number, # remove columns that are no longer necessary -pub_year, -needs_fy_sim) Now, we have a data.frame that lists every research asset, its fiscal year, and associated College(s) - where associated Colleges is a vector of length &gt;= 1. We just need to group by “College + Fiscal Year” and add up the assets. research_output_counts &lt;- data.frame( fy_college = # create &quot;College + Fiscal Year&quot; ID values unlist( lapply(1:nrow(research_outputs), function(i){ paste0(research_outputs$fiscal_year[[i]], research_outputs$unit_affiliations[[i]]) }) ), output_effort_units = unlist( sapply(1:nrow(research_outputs), function(i){ rep(research_outputs$effort_per_affiliation[[i]], length(research_outputs$unit_affiliations[[i]])) # since an fy_college value is created for each of the units affiliated with a single research output, we have to ascribe the research output&#39;s &quot;effort&quot; value as many times as there are academic units }) ), asset_id = unlist( sapply(1:nrow(research_outputs), function(i){ rep(research_outputs$asset_id[[i]], length(research_outputs$unit_affiliations[[i]])) }) ) ) Before we do the final summation, I also created a debugging data.frame to make sure the sapply(...rep(...)) behavior worked as expected, transferring the proper data in the proper order from research_outputs to research_output_counts: ### # debug - not part of pipe chain, performed before final summing research_output_debug &lt;- left_join(research_output_counts, research_outputs, by = &quot;asset_id&quot;) %&gt;% select(asset_id, fy_college, pub_year_cleaned, unit_affiliations, output_effort_units, effort_per_affiliation) %&gt;% mutate( effort_match = case_when( output_effort_units == effort_per_affiliation ~ &#39;GOOD&#39;, output_effort_units != effort_per_affiliation ~ &#39;BAD&#39;, TRUE ~ NA ), fy_date_match = case_when( as.integer(str_sub(fy_college, start = 1, end = 4)) == year(pub_year_cleaned) ~ &#39;GOOD&#39;, # for things published July - December year(pub_year_cleaned) - as.integer(str_sub(fy_college, start = 1, end = 4)) == 1 ~ &#39;GOOD&#39;, # for things published January - June year(pub_year_cleaned) - as.integer(str_sub(fy_college, start = 1, end = 4)) != 1|0 ~ &#39;BAD&#39;, TRUE ~ NA ) ) view(filter(research_output_debug, effort_match == &quot;BAD&quot; | affiliations_match == &quot;BAD&quot; | fy_college == &quot;BAD&quot;)) ### Afterward, we can (finally!), group by fiscal year and college, then sum: %&gt;% group_by(fy_college) %&gt;% summarize(total_outputs = sum(output_effort_units, na.rm = TRUE)) 2.4 The Enrollment Numbers Data.frame Enrollment numbers is reported by term and department, so the ultimate goal is to sum department numbers into College numbers, convert term to fiscal year, and to create fiscal year + College identifiers. First, we take a look at the College names in the data.frames to check if they’re consistent: print(unique(fall_enrollment$college)) print(unique(spring_enrollment$college)) Thankfully, they are, so the rest is quite straightforward: fall_enrollment &lt;- fall_enrollment %&gt;% mutate(fy_college = case_when( str_detect(college, &quot;Agricultural &amp; Life Sciences&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;cals&#39;), # &quot;Term&quot; is listed like &quot;Fall 2024&quot; in the document, so we can just take the last four characters str_detect(college, &quot;Art &amp; Architecture&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;caa&#39;), str_detect(college, &quot;Business &amp; Economics&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;cbe&#39;), str_detect(college, &quot;Education, Health &amp; Human Sci|WWAMI&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;cehhs&#39;), str_detect(college, &quot;Engineering&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;coe&#39;), str_detect(college, &quot;Letters Arts &amp; Social Sciences&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;class&#39;), str_detect(college, &quot;Law&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;law&#39;), str_detect(college, &quot;Natural Resources&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;cnr&#39;), str_detect(college, &quot;^Science$&quot;) ~ paste0(str_sub(term, start = -4, end = -1),&#39;cos&#39;), TRUE ~ NA ) ) %&gt;% group_by(fy_college) %&gt;% summarize(enrollment = sum(enrollment, na.rm = TRUE)) # generate data frame that sums enrollment numbers spring_enrollment &lt;- spring_enrollment %&gt;% mutate(fy_college = case_when( str_detect(college, &quot;Agricultural &amp; Life Sciences&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;cals&#39;), # note the addition of the as.integer(...)-1 here because the &quot;Spring 2024&quot; term is in fiscal year 2023. str_detect(college, &quot;Art &amp; Architecture&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;caa&#39;), str_detect(college, &quot;Business &amp; Economics&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;cbe&#39;), str_detect(college, &quot;Education, Health &amp; Human Sci|WWAMI&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;cehhs&#39;), str_detect(college, &quot;Engineering&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;coe&#39;), str_detect(college, &quot;Letters Arts &amp; Social Sciences&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;class&#39;), str_detect(college, &quot;Law&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;law&#39;), str_detect(college, &quot;Natural Resources&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;cnr&#39;), str_detect(college, &quot;^Science$&quot;) ~ paste0(as.integer(str_sub(term, start = -4, end = -1))-1,&#39;cos&#39;), TRUE ~ NA ) ) %&gt;% group_by(fy_college) %&gt;% summarize(enrollment = sum(enrollment, na.rm = TRUE)) 2.5 The Grant Funding Data.frame The ultimate goal for grant funding is basically identical to enrollment, so the code looks very similar. Again, we make sure the College names are consistent: print(table(grant_funding$college)) In this case, they are decidedly not, so our str_detects have to be more flexible, but still uniquely pick out the proper Colleges. Also, because grant expenditure is reported for units like the Provost Office, RCDS, etc., to keep it consistent with the enrollment data we have to omit those units (again, another non-trivial research choice): grant_funding &lt;- grant_funding %&gt;% mutate(fy_college = case_when( str_detect(college, regex(&quot;Agricultural &amp;|and Life Sciences&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;cals&#39;), str_detect(college, regex(&quot;Letters, Arts &amp;|and Social Sci&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;class&#39;), str_detect(college, regex(&quot;Art &amp;|and Architecture&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;caa&#39;), str_detect(college, regex(&quot;Business &amp;|and Economics&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;cbe&#39;), str_detect(college, regex(&quot;Education|WWAMI|Health&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;cehhs&#39;), str_detect(college, regex(&quot;Engineering&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;coe&#39;), str_detect(college, regex(&quot;law&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;law&#39;), str_detect(college, regex(&quot;Natural Resources&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;cnr&#39;), str_detect(college, regex(&quot;College of Science&quot;, ignore_case = TRUE)) ~ paste0(&#39;20&#39;,fiscal_year,&#39;cos&#39;), TRUE ~ NA )) %&gt;% group_by(fy_college) %&gt;% summarize(grant_funding = sum(grant_funding, na.rm = TRUE), grant_proposals = sum(as.integer(grant_proposals), na.rm = TRUE)) %&gt;% na.omit(fy_college) But, that’s it! We’re done with data cleaning and can move on to creating our ‘tidy’ (but actually not so tidy, as it turns out) data.frames for processing and visualization. "],["creating-populating-sql-database.html", "Chapter 3 Creating &amp; Populating SQL Database 3.1 Aggregating University-Level Variables 3.2 Aggregating Library Database Usage Data 3.3 Connecting Databases to Colleges 3.4 Creating the SQL DB and Writing to it", " Chapter 3 Creating &amp; Populating SQL Database 3.1 Aggregating University-Level Variables Since we’ve made sure grants, research outputs, and enrollment numbers all have the fiscal year + College identifier, joining those tables together is easy: university_variables &lt;- full_join(spring_enrollment, fall_enrollment, by = &quot;fy_college&quot;) %&gt;% full_join(research_output_counts, by = &quot;fy_college&quot;) %&gt;% full_join(grant_funding, by = &quot;fy_college&quot;) However, here we run into a problem about the range of the fiscal year + College identifiers. The Library’s VERSO system has assets going back to the mid-20th century all the way up to 2024. But, the raw data doesn’t include enrollment or grant numbers for that entire range. Turns out, according to… print(sort(na.omit(spring_enrollment_sums$fy_college))[1]) # sort the fy_college columns and get the first member print(sort(na.omit(spring_enrollment_sums$fy_college))[length(na.omit(spring_enrollment_sums$fy_college))]) # get the last member print(sort(na.omit(fall_enrollment_sums$fy_college))[1]) print(sort(na.omit(fall_enrollment_sums$fy_college))[length(na.omit(fall_enrollment_sums$fy_college))]) print(sort(na.omit(research_output_counts$fy_college))[1]) print(sort(na.omit(research_output_counts$fy_college))[length(na.omit(research_output_counts$fy_college))]) print(sort(na.omit(grant_funding$fy_college))[1]) print(sort(na.omit(grant_funding$fy_college))[length(na.omit(grant_funding$fy_college))]) …our raw data only covers all of these variables from FY 2015 to 2022. Rather than deal with incomplete data, I chose to omit all other years. Joining this table also forced me to make an interesting decision about enrollment. Spring and fall enrollment came in two different tables, and the numbers in each are slightly different. Given that the observational unit is the fiscal year that spans both terms, how should I handle that? Again, if we might want our assessment to tell a story about how the number of people enrolled in the University affects Library database use, it seems plausible to just add the two numbers. Originally, this seemed odd to me because many of the people enrolled in fall semester will probably also be enrolled in spring semester so it seemed like double-counting. However, similar to research outputs, we’re not interested in individual people per se, but instead the effort they might expend at the University, leading them to use Library resources. Someone enrolled for two semesters presumably expends two semesters’ worth of effort, so they can plausibly be counted twice. This is imprecise but, again, at least we’ve codified (code-ified! :D) the assumption: %&gt;% na.omit(fy_college) %&gt;% # remove anything not between 2015-2022 mutate(total_enrollment = enrollment.x+enrollment.y %&gt;% # sum spring &amp; fall enrollment select(-enrollment.x, # remove columns that are no longer necessary -enrollment.y) 3.2 Aggregating Library Database Usage Data This is where my second, and more serious, error with tidiness occurred. Essentially, the original spreadsheet contained a Library database in each row. But, the database view counts for every month from fiscal year 2015 - 2022 each had their own column. So, Web of Science would have a columns for ‘2015.07 total views’, ‘2015.08 total views’, etc. My mistake was to just sum the proper columns for each row: database_stats &lt;- left_join(az_database_usage, az_database_list, by = join_by(ID == id)) # join data.frames containing database names, id&#39;s etc., with dataframe containing usage info database_stats &lt;- database_stats %&gt;% mutate(total_views_fy15 = rowSums(database_stats[,12:23]), # sum monthly viewership columns into fiscal year viewerships total_views_fy16 = rowSums(database_stats[,24:35]), total_views_fy17 = rowSums(database_stats[,36:47]), total_views_fy18 = rowSums(database_stats[,48:59]), total_views_fy19 = rowSums(database_stats[,60:71]), total_views_fy20 = rowSums(database_stats[,72:83]), total_views_fy21 = rowSums(database_stats[,84:95]), total_views_fy22 = rowSums(database_stats[,96:107]), total_views_fy23 = rowSums(database_stats[,108:119]), remaining_views = rowSums(database_stats[,c(5:11,120:125)]), ) %&gt;% select( database_id = &quot;ID&quot;, database_name = &quot;Name&quot;, database_subjects = &quot;subjects&quot;, total_database_views = &quot;Total&quot;, total_views_fy15, total_views_fy16, total_views_fy17, total_views_fy18, total_views_fy19, total_views_fy20, total_views_fy21, total_views_fy22, # total_views_fy23, # remaining_views, ) ### # debug # database_stats &lt;- database_stats %&gt;% # mutate(math_check = rowSums(database_stats[,5:14]) == total_database_views) %&gt;% # filter(math_check == FALSE) ### What I should have done was to never join the az_database_list and az_database_usage data.frames in the first place. Instead, I should’ve linked the az_database_list table - which contained things like database_names to the database usage once via some id. I didn’t do that because of three limitations in my thinking: I unquestioningly accepted that all the database info, including views, should remain in the same data.frame (and therefore the same table in the SQL database). It seemed obvious to me that doing so would make processing easier; something I only discovered was wrong late in the project when attempting to visualize interesting relationships. I knew that having many repetitive entries within a single column can be an indicator of untidy data. Given (1) above as a constraint, the easiest way to aggregate all database view information into a single column was to pivot_longer the database table. E.g., using something like this: # an unusued version of the database_statistics data.frame. transposed_database_stats = pivot_longer(database_stats, cols = total_views_fy15:total_views_fy22) %&gt;% select(id, database_name, relevant_colleges, fiscal_year = &quot;name&quot;, views = &quot;value&quot;) %&gt;% mutate(fiscal_year = paste0(&quot;20&quot;,str_sub(fiscal_year, -2, -1)), fy_college = lapply(1:length(relevant_colleges), function(x){ lapply(relevant_colleges[[x]], function(y) paste0(fiscal_year[[x]],y)) })) However, the snippet above leads to lots of repetition in columns like database_name, id (i.e., a database id), and relevant_colleges, so it seemed wrong to me. The final limitation was that, after worrying about repetitive entries but failing to realize splitting the data.frame was a reasonable option, I failed to notice that keeping each FY’s views in its own column ultimately violates the tidy data precept of one variable per column. Regardless, I continued on my merry way until I started trying to visualize things. 3.3 Connecting Databases to Colleges In the meantime, I still need to somehow connect databases to our other observational unit of “FY + College”. To do so, I keyword search the database_stats “subjects” column - “subjects” as in “these are academic subjects where a researcher might find this database useful” - assuming there is some connection between: The work researchers do and the Colleges they are enrolled in. The subject tags and how users actually choose which database to access. %&gt;% mutate(relevant_colleges =lapply(1:nrow(database_stats), function(i){ college_list = c( # initialize a vector to contain relevant Colleges for each database case_when(str_detect(database_subjects[i], regex(&quot;Business|Economics&quot;, ignore_case = TRUE)) ~ &quot;cbe&quot;), # make keyword searches to determine relevant Colleges based on database &quot;subjects&quot; case_when(str_detect(database_subjects[i], regex(&quot;Computing|Electronics|Engineering|Chemistry|environmental&quot;, ignore_case = TRUE)) ~ &quot;coe&quot;), case_when(str_detect(database_subjects[i], regex(&quot;Education|Health|Medicine|curriculum&quot;, ignore_case = TRUE)) ~ &quot;cehhs&quot;), case_when(str_detect(database_subjects[i], regex(&quot;history|philosophy|religious|literature|anthropology|sociology|political|psychology|international|language&quot;, ignore_case = TRUE)) ~ &quot;class&quot;), case_when(str_detect(database_subjects[i], regex(&quot;architecture|music|literature&quot;, ignore_case = TRUE)) ~ &quot;caa&quot;), case_when(str_detect(database_subjects[i], regex(&quot;agricultural|family|geospatial|veterinary&quot;, ignore_case = TRUE)) ~ &quot;cals&quot;), case_when(str_detect(database_subjects[i], regex(&quot;physics|earth|chemistry|biology|geography|geology|geospatial|mathematics&quot;, ignore_case = TRUE)) ~ &quot;cos&quot;), case_when(str_detect(database_subjects[i], regex(&quot;geospatial|forestry|wildlife|fisheries|geography|environmental&quot;, ignore_case = TRUE)) ~ &quot;cnr&quot;), case_when(str_detect(database_subjects[i], regex(&quot;law&quot;, ignore_case = TRUE)) ~ &quot;law&quot;)) college_list &lt;- college_list[!is.na(college_list)] }), relevant_fycolls = lapply(1:length(relevant_colleges), function(x){ # iterate through relevant_colleges column fiscal_year = c(2015:2022) # this vector is b/c the dataset spans FY&#39;s 15-22 unlist(lapply(relevant_colleges[[x]], function(y) paste0(fiscal_year,y)))})) %&gt;% # since each entry of relevant_colleges is a vector of length n &gt;= 1, we have to iterate through them and apply a paste0 to each entry in order to generate a list of relevant FY-coll ID&#39;s filter(database_name != &quot;[Deleted]&quot;) %&gt;% # databases which the library no longer has access to were recorded as &quot;[Deleted]&quot; in the spreadsheet. Since they had incomplete data, they were removed from the analysis. filter(relevant_colleges != &quot;character(0)&quot;) # Some databases were not tagged with any subjects (subject tagging is often manually done by librarians), so they were removed from the analysis. With this information I can now generate a data.frame that records which databases are relevant to which FY’s and Colleges: database_fycoll_combos &lt;- data.frame( database_id = unlist(sapply(1:nrow(database_stats), function(i){ rep(database_stats$database_id[i], length(database_stats$relevant_fycolls[[i]])) # since databases might be relevant to more than one college and fiscal year we have to repeat their database_id entry. })), relevant_fycolls = unlist(sapply(1:nrow(database_stats), function(i) database_stats$relevant_fycolls[[i]]))) database_fycoll_combos &lt;- database_fycoll_combos %&gt;% mutate(combo_id = UUIDgenerate(n=nrow(database_fycoll_combos), output = &quot;string&quot;)) # generate universally unique id for each database FY-college combo database_stats &lt;- database_stats %&gt;% # remove the now-unnecessary columns from the database_stats data.frame. select(-relevant_colleges, -relevant_fycolls ) As a complete aside, I still do not understand why I could not declare and process the database_fycoll_combos data.frame within one pipe chain. The error I got when I tried to do so said database_fycoll_combos does not exist. Thoughts would be much appreciated! 3.4 Creating the SQL DB and Writing to it That does it for our final, processed data.frames! We can now create a SQL database to store them so we don’t have to do all that data processing every time we want to work with them or send them to collaborators/reproducers. Because none of the columns in our final data.frames have nested lists, and we didn’t see any special characters that might cause encoding issues in the data, doing so is quite straightforward. We do have some columns (e.g., “database_name”) where the lengths of entries are not standardized, but since this is a relatively small dataset we can just provide generous memory allocations without worrying about finding their exact maximum length. predicting_resource_usage_db &lt;- dbConnect(RSQLite::SQLite(), &quot;processed_data/predicting_resource_usage.db&quot;) # initialize db # create tables dbExecute(predicting_resource_usage_db, &quot;CREATE TABLE university_variables ( fy_college varchar(9) NOT NULL, total_outputs float NOT NULL, grant_funding float NOT NULL, grant_proposals integer NOT NULL, total_enrollment integer NOT NULL, PRIMARY KEY (fy_college) );&quot;) dbExecute(predicting_resource_usage_db, &quot;CREATE TABLE resource_variables ( database_id varchar(8) NOT NULL, database_name varchar(100) NOT NULL, database_subjects varchar(100), total_database_views integer, total_views_fy15 integer, total_views_fy16 integer, total_views_fy17 integer, total_views_fy18 integer, total_views_fy19 integer, total_views_fy20 integer, total_views_fy21 integer, total_views_fy22 integer,, PRIMARY KEY (database_id) );&quot;) dbExecute(predicting_resource_usage_db, &quot;CREATE TABLE database_fycoll_combos ( combo_id varchar(50), database_id varchar(8) NOT NULL, relevant_fycolls varchar(9) NOT NULL, PRIMARY KEY (combo_id), FOREIGN KEY (relevant_fycolls) REFERENCES university_variables(fy_college), FOREIGN KEY (database_id) REFERENCES resource_variables(database_id) );&quot;) # write data.frames to tables dbWriteTable(predicting_resource_usage_db, &quot;university_variables&quot;, university_variables, overwrite = TRUE) # overwrite = TRUE because, at this early proof-of-concept stage I don&#39;t anticipate the dataset getting any bigger and I didn&#39;t want to accidentally append a bunch of data to my db while repeatedly running scripts for debugging. dbWriteTable(predicting_resource_usage_db, &quot;resource_variables&quot;, database_stats, overwrite = TRUE) dbWriteTable(predicting_resource_usage_db, &quot;database_fycoll_combos&quot;, database_fycoll_combos, overwrite = TRUE) On to visualizations! "],["visualizing-the-data.html", "Chapter 4 Visualizing the Data 4.1 Loading the Data 4.2 Visualizing the Data 4.3 Conclusion", " Chapter 4 Visualizing the Data 4.1 Loading the Data We can load the data from our fancy new SQL database pretty easily: predicting_resource_usage_db &lt;- dbConnect(RSQLite::SQLite(), &quot;processed_data/predicting_resource_usage.db&quot;) univ_vars &lt;- dbGetQuery(predicting_resource_usage_db, &quot;SELECT * FROM university_variables;&quot;) database_vars &lt;- dbGetQuery(predicting_resource_usage_db, &quot;SELECT * FROM resource_variables;&quot;) combos &lt;- dbGetQuery(predicting_resource_usage_db, &quot;SELECT * FROM database_fycoll_combos;&quot;) all_data &lt;- full_join(database_vars, combos, by = &quot;database_id&quot;) %&gt;% full_join(univ_vars, join_by(&quot;relevant_fycolls&quot; == &quot;fy_college&quot;)) 4.2 Visualizing the Data Unfortunately, visualization is where untidiness comes back to bite me, and I learn a lot about the pitfalls of creating a “proof-of-concept” data processing workflow. If we recall, my two untidiness errors were: Encoding both the fiscal year and College variables in a single column. Encoding both database views and fiscal year in the same column, across multiple columns of the “database_stats” - loaded here as “database_vars” - data.frame. Error 1 ended up being not that bad, because we can do a fairly simple mutate to the data.frame to split those variables, or we can split them ad hoc when visualizing the data. E.g., this visualization of the amount of research “effort” for the College of Engineering, broken down by fiscal year: ggplot(data = filter(univ_vars, str_detect(fy_college, &quot;coe&quot;)), # split out the &quot;College&quot; part mapping = aes(x = str_sub(fy_college, 1, 4),y = total_outputs)) + # split out the FY part. geom_point() + labs(x = &quot;Fiscal Year&quot;, y = &quot;Research Output (effort units)&quot;) + ggtitle(&quot;Research Output \\&quot;Effort\\&quot;: College of Engineering&quot;) Error 2 is much more problematic. Without going back and modifying the database schema, we can fix it post-hoc by retrieving the row we want and pivoting that into a more workable dataframe: dat &lt;- filter(database_vars, str_detect(database_name, &quot;Web of Science&quot;)) dat_long &lt;- pivot_longer(dat, cols = c(5:12)) ggplot(data = dat_long, mapping = aes(x = str_sub(name, -2, -1), y = value)) + geom_point() + labs(x = &quot;Fiscal Year&quot;, y = &quot;Database Views&quot;) + ggtitle(&quot;Web of Science views by Fiscal Year&quot;) But doing so is cumbersome. It becomes more so when combining the two data.frames to explore relationships between university variables and Library database usage because the fiscal year variables are encoded in two different, mutually contradictory, ways. For instance… dat &lt;- filter(all_data, str_detect(database_name, &quot;Web of Science&quot;) &amp; str_detect(relevant_fycolls, &quot;coe&quot;)) dat_long &lt;- pivot_longer(dat, cols = c(5:12)) …produces a data.frame where entries in the “relevant_fycolls” column are repeated 7 times, one for each entry in the total databases views column (total_views_fy15, total_views_fy16, etc.). So, we have to add an additional filter to only retrieve the rows where those two columns match up: dat &lt;- filter(all_data, str_detect(database_name, &quot;Web of Science&quot;) &amp; str_detect(relevant_fycolls, &quot;coe&quot;)) dat_long &lt;- filter(pivot_longer(dat, cols = c(5:12)), as.integer(str_sub(relevant_fycolls, 3, 4)) == str_sub(name, -2, -1)) Then, we can visualize: ggplot(dat_long, aes(x = total_enrollment, y = value, label = paste0(&#39;FY&#39;,str_sub(relevant_fycolls, 3, 4)))) + geom_line() + geom_point() + geom_text_repel(size = 2.5, force_pull = 3, force = 5) + labs(x = &quot;Total Enrollment&quot;, y = &quot;Database Views&quot;)+ ggtitle(&quot;Web of Science usage by CoE Enrollment (FY15:22)&quot;) 4.3 Conclusion Though not 100% successful, this project was still valuable for a number of reasons. First and foremost, I learned that for exploratory analyses without a definite research question, brainstorming concrete research questions you may want to answer - like the above - at the beginning can help with ensuring tidy data. Second, working through a data processing workflow can be helpful for uncovering researcher degrees of freedom. In this case, navigating the ambiguities around things like… using assets or enrollment as a proxy for research “effort” which may predict library usage using database subject tags to justify connecting databases with Colleges …can help inform potential future research designs, even determining whether data quality is high enough to justify the research. Finally, I learned a bunch of R! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
